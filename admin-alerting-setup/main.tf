# main.tf

# -----------------------------------------------------------------------------
# AWS Provider Configuration
# -----------------------------------------------------------------------------
provider "aws" {
   region = "us-east-1" # IMPORTANT: Replace with your desired AWS region (e.g., "eu-west-1", "us-east-2")
}

# Data source to get the current AWS account ID, used for ARN construction
data "aws_caller_identity" "current" {}

# Random suffix for unique naming of resources like policies
resource "random_string" "suffix" {
   length  = 8
   special = false
   upper   = false
   numeric = true
}

# -----------------------------------------------------------------------------
# S3 Buckets for Logs and Athena Query Results
# -----------------------------------------------------------------------------

# 1. S3 Bucket for Raw GitHub Actions Logs
# This bucket will store the raw log files uploaded by your GitHub Actions workflow.
resource "aws_s3_bucket" "github_actions_logs" {
   bucket = "github-failed-workflow-4254" # IMPORTANT: Choose a globally unique name

   tags = {
     Name        = "GitHubActionsRawLogs"
     Environment = "Monitoring"
   }
}

# S3 Object Ownership Controls for Raw GitHub Actions Logs bucket
# This explicitly sets the bucket to "Bucket owner enforced" which disables ACLs.
resource "aws_s3_bucket_ownership_controls" "github_actions_logs_ownership" {
   bucket = aws_s3_bucket.github_actions_logs.id
   rule {
     object_ownership = "BucketOwnerEnforced"
   }
}

# Optional: S3 Lifecycle Policy for Raw Logs
# Adjust retention and transitions to cheaper storage classes based on your compliance and access needs.
resource "aws_s3_bucket_lifecycle_configuration" "github_actions_logs_lifecycle" {
   bucket = aws_s3_bucket.github_actions_logs.id

   rule {
     id     = "raw-logs-retention"
     status = "Enabled"
     filter {} # Apply rule to all objects in the bucket

     # Example: Transition to Infrequent Access after 30 days, then delete after 365 days
     # Adjust these values as per your requirements
     transition {
       days          = 30
       storage_class = "STANDARD_IA"
     }
     expiration {
       days = 365
     }
   }
}

# 2. S3 Bucket for Athena Query Results
# This dedicated bucket will store the output files generated by Athena queries.
# Crucial for cost management, as old results can accumulate.
resource "aws_s3_bucket" "athena_query_results" {
   bucket = "athena-query-results-4254" # IMPORTANT: Choose a globally unique name

   tags = {
     Name        = "AthenaQueryResults"
     Environment = "Monitoring"
   }
}

# S3 Object Ownership Controls for Athena Query Results bucket
# This explicitly sets the bucket to "Bucket owner enforced" which disables ACLs.
resource "aws_s3_bucket_ownership_controls" "athena_query_results_ownership" {
   bucket = aws_s3_bucket.athena_query_results.id
   rule {
     object_ownership = "BucketOwnerEnforced"
   }
}

# S3 Lifecycle Policy for Athena Query Results
# Highly recommended to automatically delete old results to save on S3 storage costs.
resource "aws_s3_bucket_lifecycle_configuration" "athena_results_lifecycle" {
   bucket = aws_s3_bucket.athena_query_results.id

   rule {
     id     = "delete-old-athena-results"
     status = "Enabled"
     filter {} # Apply rule to all objects in the bucket

     # Most Athena results are transient; delete after a short period (e.g., 7 days)
     expiration {
       days = 7
     }
   }
}

# -----------------------------------------------------------------------------
# AWS Glue Data Catalog (Athena Table Definition)
# -----------------------------------------------------------------------------

# AWS Glue Catalog Database for Athena
# This creates the database where your Athena table will be defined.
# If the database already exists (e.g., from manual creation), you have two options:
# Option A (Recommended): Import it into Terraform state:
#   1. Temporarily comment out this block.
#   2. Run: terraform import aws_glue_catalog_database.github_actions_logs_db <YOUR_AWS_ACCOUNT_ID>:github_actions_logs
#   3. Uncomment this block.
#   4. Run terraform plan/apply again.
# Option B (Simpler, but not fully Terraform managed): Remove this block entirely if you're fine with it being manually managed.
resource "aws_glue_catalog_database" "github_actions_logs_db" {
   name        = "github_actions_logs_2"
   catalog_id = data.aws_caller_identity.current.account_id
}

# AWS Glue Catalog Table for Workflow Failure Logs
# This defines the schema for your raw log files in S3, allowing Athena to query them.
resource "aws_glue_catalog_table" "workflow_failure_logs_table" {
   name          = "workflow_failure_logs"
   database_name = aws_glue_catalog_database.github_actions_logs_db.name
   catalog_id    = data.aws_caller_identity.current.account_id

   # Storage Descriptor defines how Athena reads the data from S3
   storage_descriptor {
     location      = "s3://${aws_s3_bucket.github_actions_logs.id}/path/to/your/logs/" # IMPORTANT: Adjust this path to where your actual log files are (e.g., "s3://your-bucket/workflow-runs/")
     input_format  = "org.apache.hadoop.mapred.TextInputFormat"
     output_format = "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"
     
     # Corrected: serde_info changed to ser_de_info
     ser_de_info {
       name = "LazySimpleSerDe"
       serialization_library = "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"
       parameters = {
         "serialization.format" = "1"
         "field.delim"          = "\t" # Adjust if your logs use a different delimiter (e.g., comma, space)
       }
     }

     # Define the columns in your log_entry. We'll initially treat the whole line as a string.
     # Parsing into structured fields will happen in Grafana's SQL query.
     columns {
       name = "log_entry"
       type = "string"
     }

     # If your logs are partitioned (e.g., by year, month, day), define partitions here.
     # Example for year/month/day partitioning:
     # partitioned_by = ["year", "month", "day"]
     # If you add partitions here, you'll need to run MSCK REPAIR TABLE in Athena
     # after new data is added to S3, or set up a Glue Crawler to do it automatically.
   }

   table_type = "EXTERNAL_TABLE" # Essential for S3-backed tables

   # Parameters for the table, including classification for Glue
   parameters = {
     "classification" = "text" # Or "json", "text", etc., based on your actual log file format
     "EXTERNAL"       = "TRUE"
   }

   # Explicit dependency to ensure S3 bucket is created before the Glue table
   depends_on = [
     aws_s3_bucket.github_actions_logs
   ]
}