# main.tf

# -----------------------------------------------------------------------------
# AWS Provider Configuration
# -----------------------------------------------------------------------------
provider "aws" {
  region = "us-east-1" # IMPORTANT: Replace with your desired AWS region (e.g., "eu-west-1", "us-east-2")
}

# Data source to get the current AWS account ID, used for ARN construction
data "aws_caller_identity" "current" {}

# Random suffix for unique naming of resources like policies
resource "random_string" "suffix" {
  length  = 8
  special = false
  upper   = false
  numeric = true
}

# -----------------------------------------------------------------------------
# S3 Buckets for Logs and Athena Query Results
# -----------------------------------------------------------------------------

# 1. S3 Bucket for Raw GitHub Actions Logs
# This bucket will store the raw log files uploaded by your GitHub Actions workflow.
resource "aws_s3_bucket" "github_actions_logs" {
  bucket = "your-github-actions-logs-bucket-name-unique" # IMPORTANT: Choose a globally unique name

  tags = {
    Name        = "GitHubActionsRawLogs"
    Environment = "Monitoring"
  }
}

# S3 Object Ownership Controls for Raw GitHub Actions Logs bucket
# This explicitly sets the bucket to "Bucket owner enforced" which disables ACLs.
resource "aws_s3_bucket_ownership_controls" "github_actions_logs_ownership" {
  bucket = aws_s3_bucket.github_actions_logs.id
  rule {
    object_ownership = "BucketOwnerEnforced"
  }
}

# Optional: S3 Lifecycle Policy for Raw Logs
# Adjust retention and transitions to cheaper storage classes based on your compliance and access needs.
resource "aws_s3_bucket_lifecycle_configuration" "github_actions_logs_lifecycle" {
  bucket = aws_s3_bucket.github_actions_logs.id

  rule {
    id     = "raw-logs-retention"
    status = "Enabled"
    filter {} # Apply rule to all objects in the bucket

    # Example: Transition to Infrequent Access after 30 days, then delete after 365 days
    # Adjust these values as per your requirements
    transition {
      days          = 30
      storage_class = "STANDARD_IA"
    }
    expiration {
      days = 365
    }
  }
}

# 2. S3 Bucket for Athena Query Results
# This dedicated bucket will store the output files generated by Athena queries.
# Crucial for cost management, as old results can accumulate.
resource "aws_s3_bucket" "athena_query_results" {
  bucket = "your-athena-query-results-bucket-name-unique" # IMPORTANT: Choose a globally unique name

  tags = {
    Name        = "AthenaQueryResults"
    Environment = "Monitoring"
  }
}

# S3 Object Ownership Controls for Athena Query Results bucket
# This explicitly sets the bucket to "Bucket owner enforced" which disables ACLs.
resource "aws_s3_bucket_ownership_controls" "athena_query_results_ownership" {
  bucket = aws_s3_bucket.athena_query_results.id
  rule {
    object_ownership = "BucketOwnerEnforced"
  }
}

# S3 Lifecycle Policy for Athena Query Results
# Highly recommended to automatically delete old results to save on S3 storage costs.
resource "aws_s3_bucket_lifecycle_configuration" "athena_results_lifecycle" {
  bucket = aws_s3_bucket.athena_query_results.id

  rule {
    id     = "delete-old-athena-results"
    status = "Enabled"
    filter {} # Apply rule to all objects in the bucket

    # Most Athena results are transient; delete after a short period (e.g., 7 days)
    expiration {
      days = 7
    }
  }
}

# -----------------------------------------------------------------------------
# IAM Role, Instance Profile, and Policies for EC2 (Grafana)
# -----------------------------------------------------------------------------

# IAM Role that the EC2 instance will assume.
# This role grants permissions to interact with AWS services.
resource "aws_iam_role" "ec2_combined_role" {
  name = "EC2CombinedRoleForGrafanaAthena-${random_string.suffix.result}" # Unique name

  # Policy that allows the EC2 service to assume this role.
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Action    = "sts:AssumeRole",
        Effect    = "Allow",
        Principal = {
          Service = "ec2.amazonaws.com"
        },
      },
    ],
  })

  tags = {
    Name = "EC2CombinedRoleForGrafanaAthena"
  }
}

# IAM Instance Profile to link the IAM role to the EC2 instance.
# EC2 instances use instance profiles to get their IAM permissions.
resource "aws_iam_instance_profile" "ec2_combined_profile" {
  name = "EC2CombinedInstanceProfileForGrafanaAthena-${random_string.suffix.result}" # Unique name
  role = aws_iam_role.ec2_combined_role.name

  tags = {
    Name = "EC2CombinedInstanceProfileForGrafanaAthena"
  }
}

# Attach AmazonAthenaFullAccess managed policy
# This policy grants comprehensive permissions to perform actions with Athena.
resource "aws_iam_role_policy_attachment" "athena_policy_attachment" {
  role       = aws_iam_role.ec2_combined_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonAthenaFullAccess"
}

# Attach AmazonS3ReadOnlyAccess managed policy
# This policy allows reading data from S3, including your raw GitHub Actions logs.
resource "aws_iam_role_policy_attachment" "s3_readonly_policy_attachment" {
  role       = aws_iam_role.ec2_combined_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
}

# Attach AWSGlueConsoleFullAccess managed policy
# This policy grants access to AWS Glue, where Athena table definitions (Data Catalog) reside.
resource "aws_iam_role_policy_attachment" "glue_policy_attachment" {
  role       = aws_iam_role.ec2_combined_role.name
  policy_arn = "arn:aws:iam::aws:policy/AWSGlueConsoleFullAccess"
}

# Custom S3 policy for Athena Query Results bucket
# This policy grants specific permissions for Athena to write and Grafana to read
# query results in the dedicated S3 bucket.
resource "aws_iam_policy" "athena_results_s3_policy" {
  name        = "AthenaQueryResultsS3Policy-${random_string.suffix.result}" # Unique name
  description = "Permissions for Athena to write and Grafana to read query results in a specific S3 bucket."

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect   = "Allow"
        Action   = [
          "s3:PutObject",    # Athena writes query results
          "s3:GetObject",    # Grafana reads query results
          "s3:DeleteObject"  # Athena might need this for temporary files/cleanup
        ]
        Resource = "${aws_s3_bucket.athena_query_results.arn}/*" # Specific path within the bucket
      },
      {
        Effect   = "Allow"
        Action   = [
          "s3:ListBucket" # Needed for listing contents within the bucket/prefix
        ]
        Resource = aws_s3_bucket.athena_query_results.arn
        Condition = {
          "StringLike": {
            "s3:prefix": [
              "athena-query-results/*", # Or the specific prefix Athena uses
              "" # Allow listing the root of the bucket
            ]
          }
        }
      }
    ]
  })
}

# Attach the custom S3 policy to the EC2 role
resource "aws_iam_role_policy_attachment" "athena_results_s3_policy_attachment" {
  role       = aws_iam_role.ec2_combined_role.name
  policy_arn = aws_iam_policy.athena_results_s3_policy.arn
}

# -----------------------------------------------------------------------------
# EC2 Instance for Grafana
# -----------------------------------------------------------------------------

# Security Group for the Grafana EC2 instance
# Allows SSH access (port 22) and Grafana UI access (port 3000).
resource "aws_security_group" "grafana_sg" {
  name        = "grafana-ec2-sg-${random_string.suffix.result}" # Unique name
  description = "Allow SSH and Grafana access"

  # Ingress rule for SSH (port 22)
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"] # WARNING: For production, restrict this to your specific IP address or a known CIDR block
    description = "Allow SSH access"
  }

  # Ingress rule for Grafana UI (port 3000)
  ingress {
    from_port   = 3000
    to_port     = 3000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"] # WARNING: For production, restrict this to your specific IP address or a known CIDR block
    description = "Allow Grafana access"
  }

  # Egress rule to allow all outbound traffic (Grafana needs to talk to AWS services)
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1" # All protocols
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow all outbound traffic"
  }

  tags = {
    Name = "GrafanaEC2SecurityGroup"
  }
}

# EC2 Instance to run Grafana
resource "aws_instance" "grafana_server" {
  # ami = "ami-053b04d416b25121a" # Example: Ubuntu Server 22.04 LTS (HVM), SSD Volume Type - us-east-1
  # IMPORTANT: Find the latest AMI for your chosen region and OS.
  # You can find AMIs in the EC2 console when launching a new instance.
  ami           = "ami-053b0d53c279acc90" # Replace with a valid AMI ID for your region (e.g., Ubuntu 22.04 LTS)
  instance_type = "t3.micro" # Or t3.medium for better performance
  vpc_security_group_ids = [aws_security_group.grafana_sg.id] # Reference the SG by ID
  iam_instance_profile = aws_iam_instance_profile.ec2_combined_profile.name # Attach the IAM instance profile

  # User data script to install Docker and run Grafana container on boot.
  user_data = <<-EOF
              #!/bin/bash
              echo "Starting user data script..."
              sudo apt-get update -y
              sudo apt-get install -y docker.io
              sudo systemctl start docker
              sudo systemctl enable docker
              echo "Docker installed and started."
              # Run Grafana container, mapping port 3000
              sudo docker run -d -p 3000:3000 --name grafana grafana/grafana-oss:latest
              echo "Grafana container started."
              EOF

  tags = {
    Name        = "GrafanaServer"
    Environment = "Monitoring"
  }
}

# Output the public IP address of the Grafana EC2 instance
output "grafana_public_ip" {
  description = "The public IP address of the Grafana EC2 instance"
  value       = aws_instance.grafana_server.public_ip
}

# -----------------------------------------------------------------------------
# AWS Glue Data Catalog (Athena Table Definition)
# -----------------------------------------------------------------------------

# AWS Glue Catalog Database for Athena
# This creates the database where your Athena table will be defined.
# If the database already exists (e.g., from manual creation), you have two options:
# Option A (Recommended): Import it into Terraform state:
#   1. Temporarily comment out this block.
#   2. Run: terraform import aws_glue_catalog_database.github_actions_logs_db <YOUR_AWS_ACCOUNT_ID>:github_actions_logs
#   3. Uncomment this block.
#   4. Run terraform plan/apply again.
# Option B (Simpler, but not fully Terraform managed): Remove this block entirely if you're fine with it being manually managed.
resource "aws_glue_catalog_database" "github_actions_logs_db" {
  name       = "github_actions_logs_2"
  catalog_id = data.aws_caller_identity.current.account_id
}

# AWS Glue Catalog Table for Workflow Failure Logs
# This defines the schema for your raw log files in S3, allowing Athena to query them.
resource "aws_glue_catalog_table" "workflow_failure_logs_table" {
  name          = "workflow_failure_logs"
  database_name = aws_glue_catalog_database.github_actions_logs_db.name
  catalog_id    = data.aws_caller_identity.current.account_id

  # Storage Descriptor defines how Athena reads the data from S3
  storage_descriptor {
    location      = "s3://${aws_s3_bucket.github_actions_logs.id}/path/to/your/logs/" # IMPORTANT: Adjust this path to where your actual log files are (e.g., "s3://your-bucket/workflow-runs/")
    input_format  = "org.apache.hadoop.mapred.TextInputFormat"
    output_format = "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"
    
    # Corrected: serde_info changed to ser_de_info
    ser_de_info {
      name = "LazySimpleSerDe"
      serialization_library = "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"
      parameters = {
        "serialization.format" = "1"
        "field.delim"          = "\t" # Adjust if your logs use a different delimiter (e.g., comma, space)
      }
    }

    # Define the columns in your log_entry. We'll initially treat the whole line as a string.
    # Parsing into structured fields will happen in Grafana's SQL query.
    columns {
      name = "log_entry"
      type = "string"
    }

    # If your logs are partitioned (e.g., by year, month, day), define partitions here.
    # Example for year/month/day partitioning:
    # partitioned_by = ["year", "month", "day"]
    # If you add partitions here, you'll need to run MSCK REPAIR TABLE in Athena
    # after new data is added to S3, or set up a Glue Crawler to do it automatically.
  }

  table_type = "EXTERNAL_TABLE" # Essential for S3-backed tables

  # Parameters for the table, including classification for Glue
  parameters = {
    "classification" = "csv" # Or "json", "text", etc., based on your actual log file format
    "EXTERNAL"       = "TRUE"
  }

  # Explicit dependency to ensure S3 bucket is created before the Glue table
  depends_on = [
    aws_s3_bucket.github_actions_logs
  ]
}
